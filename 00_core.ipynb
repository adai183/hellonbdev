{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello nbdev\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "from typing import List\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tensorflow_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def say_hello(to):\n",
    "    \"Say hello to somebody\"\n",
    "    return(f'Hello {to}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to say hello to someone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Sylvain'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "say_hello(\"Sylvain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_eq(say_hello(\"Jeremy\"), \"Hello Jeremy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HelloSayer:\n",
    "    \"Say hello to `to` using `say_hello`\"\n",
    "    def __init__(self, to): self.to = to\n",
    "    def say(self):\n",
    "        \"do the saying\"\n",
    "        return say_hello(self.to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"HelloSayer.say\" class=\"doc_header\"><code>HelloSayer.say</code><a href=\"__main__.py#L5\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>HelloSayer.say</code>()\n",
       "\n",
       "do the saying"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HelloSayer.say)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Bichec'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = HelloSayer(\"Bichec\")\n",
    "o.say()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class UseVectorizer:\n",
    "    \"\"\"Wrapper for using Googles's Universal sentence encoder from Tensorflow Hub. \n",
    "    \n",
    "    We have two types of encoders:\n",
    "        * the regular USE\n",
    "        * a dual encoder version for Q&A with a specific encoder for questions and responses\n",
    "    \n",
    "    **Args:**\n",
    "        \n",
    "        model_name(str): Name of the model to be loaded\n",
    "    \n",
    "    **Attributes:**\n",
    "    \n",
    "         MODELS(dict): Constant, that stores mapping between the model name and the TF Hub download URL\n",
    "         \n",
    "         model: Model loaded from TF Hub\n",
    "    \"\"\"\n",
    "\n",
    "    MODELS = {\n",
    "        \"universal-sentence-encoder\": \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "        \"universal-sentence-encoder-large\": \"https://tfhub.dev/google/universal-sentence-encoder-large/5\",\n",
    "        \"universal-sentence-encoder-multilingual\": \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\",\n",
    "        \"universal-sentence-encoder-multilingual-large\": \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\",\n",
    "        \"universal-sentence-encoder-qa\": \"https://tfhub.dev/google/universal-sentence-encoder-qa/3\",\n",
    "        \"universal-sentence-encoder-multilingual-qa\": \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, model_name: str = \"universal-sentence-encoder\") -> None:\n",
    "        self.model = hub.load(self.MODELS[model_name])\n",
    "\n",
    "    def encode(\n",
    "        self, texts: List, encoder_type: str = \"question\", response_contexts: List = []\n",
    "    ) -> tf.Tensor:\n",
    "        \"\"\"Encodes an input text.\n",
    "        \n",
    "        **Args**:\n",
    "            \n",
    "            texts(List): List of strings to encode\n",
    "            \n",
    "            encoder_type(str): Only for Q&A models; \n",
    "                               choose question or response encoder.\n",
    "            \n",
    "            response_contexts: Only for the response encoder of a Q&A models;\n",
    "                               usually the text around the answer text, for example it could be \n",
    "                               2 sentneces before plus 2 sentences after, it could also be \n",
    "                               the paragraph conaining the answer text. If no context is included, \n",
    "                               the response field is duplicated.\n",
    "        **Returns**:\n",
    "            2D Tensor containing vector representations of the input\n",
    "        \"\"\"\n",
    "        # handle non Q&A models\n",
    "        if \"question_encoder\" not in self.model.signatures:\n",
    "            return self.model(texts)\n",
    "        # handle Q&A question encoder\n",
    "        if encoder_type == \"question\":\n",
    "            texts = tf.constant(texts)\n",
    "            return self.model.signatures[\"question_encoder\"](texts)[\"outputs\"]\n",
    "        # handle Q&A response encoder\n",
    "        if encoder_type == \"response\":\n",
    "            texts = tf.constant(texts)\n",
    "            if response_contexts:\n",
    "                response_contexts = tf.constant(response_contexts)\n",
    "            # duplicate response in case there is no context as suggested in the model docs\n",
    "            else:\n",
    "                response_contexts = texts\n",
    "            return self.model.signatures[\"response_encoder\"](\n",
    "                input=texts, context=response_contexts\n",
    "            )[\"outputs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"UseVectorizer.encode\" class=\"doc_header\"><code>UseVectorizer.encode</code><a href=\"__main__.py#L32\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>UseVectorizer.encode</code>(**`texts`**:`List`\\[`T`\\], **`encoder_type`**:`str`=*`'question'`*, **`response_contexts`**:`List`\\[`T`\\]=*`[]`*)\n",
       "\n",
       "Encodes an input text.\n",
       "\n",
       "**Args**:\n",
       "    \n",
       "    texts(List): List of strings to encode\n",
       "    \n",
       "    encoder_type(str): Only for Q&A models; \n",
       "                       choose question or response encoder.\n",
       "    \n",
       "    response_contexts: Only for the response encoder of a Q&A models;\n",
       "                       usually the text around the answer text, for example it could be \n",
       "                       2 sentneces before plus 2 sentences after, it could also be \n",
       "                       the paragraph conaining the answer text. If no context is included, \n",
       "                       the response field is duplicated.\n",
       "**Returns**:\n",
       "    2D Tensor containing vector representations of the input"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(UseVectorizer.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 (nbdev)",
   "language": "python",
   "name": "nbdev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
